from pyspark.sql import SparkSession, Window, functions as F, types as T
import pandas as pd
from collections import defaultdict, OrderedDict

# ============================================================
#  BOM Builder (Forward + Reverse Where-Used)
# ============================================================



# ----------------------------------------------------------------
# Spark session & basic config
# ----------------------------------------------------------------
spark = SparkSession.builder.getOrCreate()
spark.conf.set("spark.sql.shuffle.partitions", 200)  # tune as needed


# ============================================================
#  FORWARD BOM EXPLOSION (unchanged logic)
# ============================================================

def build_boms():
    query = """
        SELECT FINPartNum, PartPath, QtyPath
        FROM py_fullwhereused
        WHERE PartPath IS NOT NULL
    """
    df = spark.sql(query)

    # Repartition for decent parallelism
    df = df.repartition("FINPartNum").persist()

    # ----------------------
    # pandas UDF per FINPartNum
    # ----------------------
    def build_bom(pdf: pd.DataFrame) -> pd.DataFrame:
        if pdf.empty:
            return pd.DataFrame(columns=[
                "FINPartNum", "ParentPart", "Part", "BOMLevel", "Qty", "OrderIndex"
            ])

        fin = pdf["FINPartNum"].iloc[0]

        # adjacency
        tree = defaultdict(OrderedDict)   # parent -> OrderedDict(child -> order_idx)
        qty_lookup = {}                   # (parent, child) -> qty
        root_qty = {}
        order_counter = 0
        seen_roots, seen_root_set = [], set()
        all_parents, all_children = set(), set()

        def sfloat(x):
            try:
                return float(x)
            except:
                return 1.0

        # build adjacency
        for _, row in pdf.iterrows():
            parts = [p.strip() for p in (row.PartPath or "").split("|") if p.strip()]
            qtys = [sfloat(q) for q in (row.QtyPath or "").split("|")] if row.QtyPath else []
            if len(qtys) < len(parts):
                qtys += [1.0] * (len(parts) - len(qtys))

            if parts:
                top = parts[0]
                if top not in seen_root_set:
                    seen_roots.append(top)
                    seen_root_set.add(top)
                root_qty.setdefault(top, qtys[0])

            for i in range(1, len(parts)):
                parent, child = parts[i-1], parts[i]
                all_parents.add(parent)
                all_children.add(child)
                if child not in tree[parent]:
                    tree[parent][child] = order_counter
                    order_counter += 1
                qty_lookup[(parent, child)] = qtys[i]

        # roots = top assemblies within this FIN
        roots = [p for p in seen_roots if p in all_parents and p not in all_children]
        if not roots:
            roots = seen_roots.copy()

        # Depth-first traversal with “leaves first, then subassemblies”
        rows = []
        order_counter = 0

        def traverse(node, level, parent):
            nonlocal order_counter
            qty_val = root_qty.get(node, 1.0) if level == 0 else qty_lookup.get((parent, node), 1.0)
            rows.append((fin, parent, node, level, float(qty_val), order_counter))
            order_counter += 1

            children = sorted(tree.get(node, {}).items(), key=lambda x: x[1])
            leafs = [c for c, _ in children if c not in tree]
            parents = [c for c, _ in children if c in tree]

            for c in leafs:
                traverse(c, level + 1, node)
            for c in parents:
                traverse(c, level + 1, node)

        for r in roots:
            traverse(r, 0, None)

        return pd.DataFrame(rows, columns=[
            "FINPartNum", "ParentPart", "Part", "BOMLevel", "Qty", "OrderIndex"
        ])

    schema = T.StructType([
        T.StructField("FINPartNum", T.StringType()),
        T.StructField("ParentPart", T.StringType()),
        T.StructField("Part", T.StringType()),
        T.StructField("BOMLevel", T.IntegerType()),
        T.StructField("Qty", T.DoubleType()),
        T.StructField("OrderIndex", T.LongType()),
    ])

    forward_bom = (
        df.groupBy("FINPartNum")
          .applyInPandas(build_bom, schema=schema)
          .orderBy("FINPartNum", "OrderIndex")
    )

    # Add DisplayOrder per FINPartNum for Power BI
    window = Window.partitionBy("FINPartNum").orderBy("OrderIndex")
    forward_bom = forward_bom.withColumn("DisplayOrder", F.row_number().over(window))

    # Final column order
    forward_bom = forward_bom.select(
        "DisplayOrder", "FINPartNum", "ParentPart", "Part", "BOMLevel", "Qty"
    ).persist()

    return forward_bom


# ============================================================
#  REVERSE WHERE-USED (per FINPartNum via applyInPandas)
# ============================================================

def build_reverse_whereused_grouped(forward_bom, target_fins=None):
    """
    Reverse where-used using Pandas graph logic per FINPartNum,
    executed in parallel via groupBy().applyInPandas().
    """

    # Optionally filter to a subset of FINPartNum for testing / performance
    fwd = forward_bom
    if target_fins:
        fwd = fwd.filter(F.col("FINPartNum").isin(target_fins))

    # We'll only need these columns inside the Pandas UDF
    base = fwd.select("FINPartNum", "ParentPart", "Part", "BOMLevel", "Qty")

    # ----- Pandas UDF: one FINPartNum partition at a time -----
    def reverse_for_fin(pdf: pd.DataFrame) -> pd.DataFrame:
        if pdf.empty:
            return pd.DataFrame(columns=["FINPartNum", "BOMLevel", "ParentPart", "Part", "Qty"])

        fin = pdf["FINPartNum"].iloc[0]

        # Only consider valid parent-child rows
        sub = pdf[(pdf["ParentPart"].notna()) & (pdf["Part"].notna())]
        if sub.empty:
            return pd.DataFrame(columns=["FINPartNum", "BOMLevel", "ParentPart", "Part", "Qty"])

        # Build parent -> [(child, qty)] and child -> [(parent, qty)]
        children_map = defaultdict(list)
        parents_map = defaultdict(list)

        for _, r in sub.iterrows():
            parent = r["ParentPart"]
            child = r["Part"]
            qty = r["Qty"]
            children_map[parent].append((child, qty))
            parents_map[child].append((parent, qty))

        # All children in this FIN
        target_children = sub["Part"].unique().tolist()

        from collections import deque
        results = []

        for child in target_children:
            if child not in parents_map:
                continue

            queue = deque()
            visited = set()

            # Seed: direct parents (plevel 0, with qty, is_direct=True)
            for parent, qty in parents_map[child]:
                queue.append((parent, 0, qty, True))
                visited.add(parent)

            while queue:
                parent, plevel, qty, is_direct = queue.popleft()

                # Record this parent-child relationship
                results.append((
                    fin,
                    plevel,                # temporary BOMLevel, will invert later
                    parent,
                    child,
                    qty if is_direct else None   # Qty only on deepest-level parent
                ))

                # Go up another level, grandparents of this parent
                if parent in parents_map:
                    for gparent, _ in parents_map[parent]:
                        if gparent not in visited:
                            visited.add(gparent)
                            queue.append((gparent, plevel + 1, None, False))

        if not results:
            return pd.DataFrame(columns=["FINPartNum", "BOMLevel", "ParentPart", "Part", "Qty"])

        rev_df = pd.DataFrame(results, columns=[
            "FINPartNum", "BOMLevel", "ParentPart", "Part", "Qty"
        ])

        # Invert BOMLevel so 0 = TOP parent, increasing toward child:
        # if original levels are: 0 (closest), 1, 2 (top),
        # we want: 2 (closest), 1, 0 (top).
        max_lvl = rev_df.groupby(["FINPartNum", "Part"])["BOMLevel"].transform("max")
        rev_df["BOMLevel"] = max_lvl - rev_df["BOMLevel"]

        # Final column order (already correct)
        rev_df = rev_df[["FINPartNum", "BOMLevel", "ParentPart", "Part", "Qty"]]

        return rev_df

    # Define schema for the reverse where-used output
    reverse_schema = T.StructType([
        T.StructField("FINPartNum", T.StringType()),
        T.StructField("BOMLevel", T.IntegerType()),
        T.StructField("ParentPart", T.StringType()),
        T.StructField("Part", T.StringType()),
        T.StructField("Qty", T.DoubleType()),
    ])

    reverse_bom = (
        base.groupBy("FINPartNum")
            .applyInPandas(reverse_for_fin, schema=reverse_schema)
    )

    return reverse_bom


# ============================================================
#  EXAMPLE USAGE
# ============================================================

# Build forward BOM (same as before)
forward_bom = build_boms()

# Option 1: Full reverse where-used for all FINPartNum (might be heavy)
reverse_bom = build_reverse_whereused_grouped(forward_bom)

# Option 2: Test on a subset of FINs (recommended first)
#reverse_bom = build_reverse_whereused_grouped(
    #forward_bom,
    #target_fins=["SA49299.904", "SA50579.904"]  # adjust/remove as needed
#)

# You can register views for Power BI / SQL
#forward_bom.createOrReplaceTempView("forward_bom")
#reverse_bom.createOrReplaceTempView("reverse_bom")


reverse_bom = (
        reverse_bom
        .orderBy("FINPartNum", "Part", "BOMLevel")  # 0,1,2,... for each FIN/Part
    )


# ============================================================
#  ADD PART DESCRIPTION TO reverse_bom
# ============================================================

from pyspark.sql import functions as F

# 1) Load base tables
df_invent = spark.read.table("inventtable").select(
    F.col("itemid"),
    F.col("product")
)

df_prod = spark.read.table("ecoresproduct").select(
    F.col("recid").alias("product_recid")
)

df_trans = spark.read.table("ecoresproducttranslation").select(
    F.col("product").alias("translation_product"),
    F.col("description").alias("TransDescription"),
    F.col("name").alias("TransName")
)

# 2) Join reverse_bom → inventtable
rev_with_invent = (
    reverse_bom
        .join(df_invent, reverse_bom.ParentPart == df_invent.itemid, how="left")
        .drop("itemid")
)

# 3) Join inventtable → ecoresproduct
rev_with_prod = (
    rev_with_invent
        .join(df_prod,
              rev_with_invent.product == df_prod.product_recid,
              how="left")
        .drop("product_recid")
)

# 4) Join ecoresproduct → ecoresproducttranslation
rev_with_trans = (
    rev_with_prod
        .join(df_trans,
              rev_with_prod.product == df_trans.translation_product,
              how="left")
        .drop("translation_product")
)

# 5) Build final Description (NULL-safe):
#    1) ecoresproducttranslation.description
#    2) ecoresproducttranslation.name
reverse_bom_with_description = (
    rev_with_trans
        .withColumn(
            "Description",
            F.coalesce(F.col("TransDescription"), F.col("TransName"))
        )
        .drop("TransDescription", "TransName", "product")   # clean extras
)

# 6) Final output column order
reverse_bom_with_description = reverse_bom_with_description.select(
    "FINPartNum",
    "BOMLevel",
    "ParentPart",
    "Description",
    "Part",
    "Qty"
)

# Preview
reverse_bom_with_description.show(50, truncate=False)
